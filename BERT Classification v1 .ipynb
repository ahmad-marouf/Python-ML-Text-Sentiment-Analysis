{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e45eec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e7a31",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb04cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8367fa",
   "metadata": {},
   "source": [
    "**Check the head of df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057ea4b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d5c5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469b9d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c495d7",
   "metadata": {},
   "source": [
    "## Data Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d98ddb",
   "metadata": {},
   "source": [
    "#### Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ce72ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(rating):\n",
    "  if rating == 'positive':\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "df['sentiment'] = df.sentiment.apply(to_sentiment)\n",
    "\n",
    "class_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ddcc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    17500\n",
      "0    17500\n",
      "Name: sentiment, dtype: int64\n",
      "0    2500\n",
      "1    2500\n",
      "Name: sentiment, dtype: int64\n",
      "0    5000\n",
      "1    5000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=123, stratify= df.sentiment)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.125, random_state=123, stratify= df_train.sentiment)\n",
    "\n",
    "\n",
    "print(df_train.sentiment.value_counts())\n",
    "print(df_val.sentiment.value_counts())\n",
    "print(df_test.sentiment.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7715f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(columns='sentiment')\n",
    "# y = df['sentiment']\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify= y)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=123, stratify= y_train)\n",
    "\n",
    "\n",
    "# print(y_train.value_counts())\n",
    "# print(y_val.value_counts())\n",
    "# print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c728f8",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2c63e0",
   "metadata": {},
   "source": [
    "#### Original Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33056edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_train['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b6bc9",
   "metadata": {},
   "source": [
    "#### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "003a1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_lowerCase = df_train.review.str.lower()\n",
    "# reviews_lowerCase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ed412a",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f99ff74b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [nltk.sent_tokenize(item) for item in reviews_lowerCase]\n",
    "# tokens = [nltk.word_tokenize(item) for item in reviews_lowerCase]\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8860d",
   "metadata": {},
   "source": [
    "#### Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "771bff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "# res=[regex.sub('', word) for words in tokens for word in words if not regex.sub('', word) == '']\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353273b7",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96cadd23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.append('via')\n",
    "# words = [token for token in res if token not in stop_words]\n",
    "# words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7857d245",
   "metadata": {},
   "source": [
    "#### Removing Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3e9a964",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# regex = re.compile('http\\S+')\n",
    "# tokens_without_links = [regex.sub('', word) for word in words if not regex.sub('', word) == '' and not word.startswith('tc')]\n",
    "# tokens_without_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0f4e9",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34071760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = PorterStemmer()\n",
    "# stemmed_words = [stemmer.stem(word) for word in tokens_without_links]\n",
    "# stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31003f47",
   "metadata": {},
   "source": [
    "#### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d77d69bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "# lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41462873",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd5a9812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when was i last outside? i am stuck at home for 2 weeks.\n",
      "['when', 'was', 'i', 'last', 'outside', '?', 'i', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n",
      "['when', 'was', 'i', 'last', 'outside', 'i', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks']\n",
      "['last', 'outside', 'stuck', 'home', '2', 'weeks']\n",
      "['last', 'outside', 'stuck', 'home', '2', 'weeks']\n",
      "['last', 'outside', 'stuck', 'home', '2', 'week']\n",
      "last outside stuck home 2 week\n"
     ]
    }
   ],
   "source": [
    "sample_str = 'When was I last outside? I am stuck at home for 2 weeks.'\n",
    "\n",
    "# set all as lowercase\n",
    "lowerCase = sample_str.lower()\n",
    "\n",
    "# tokenize\n",
    "tokens = nltk.word_tokenize(lowerCase)\n",
    "\n",
    "# remove punctuation\n",
    "regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "tokens_NoPunct = [regex.sub('', word) for word in tokens if not regex.sub('', word) == '']\n",
    "\n",
    "# removing stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('via')\n",
    "tokens_NoStopWords = [token for token in tokens_NoPunct if token not in stop_words]\n",
    "\n",
    "# removing links\n",
    "regex = re.compile('http\\S+')\n",
    "tokens_NoLinks = [regex.sub('', word) for word in tokens_NoStopWords if not regex.sub('', word) == '' and not word.startswith('tc')]\n",
    "\n",
    "# Lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_NoLinks]\n",
    "\n",
    "# Detokenizing\n",
    "untokenized = TreebankWordDetokenizer().detokenize(lemmatized_words)\n",
    "\n",
    "\n",
    "print(lowerCase)\n",
    "print(tokens)\n",
    "print(tokens_NoPunct)\n",
    "print(tokens_NoStopWords)\n",
    "print(tokens_NoLinks)\n",
    "print(lemmatized_words)\n",
    "print(untokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e27904bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(df):\n",
    "    processed_Reviews = []\n",
    "    for review in df.review:\n",
    "        # original\n",
    "        # print(review)\n",
    "\n",
    "        # set all as lowercase\n",
    "        lowerCase = review.lower()\n",
    "\n",
    "        # tokenize\n",
    "        tokens = nltk.word_tokenize(lowerCase)\n",
    "\n",
    "        # remove punctuation\n",
    "        regex = re.compile(f'[{re.escape(string.punctuation)}]')\n",
    "        tokens_NoPunct = [regex.sub('', word) for word in tokens if not regex.sub('', word) == '']\n",
    "\n",
    "        # removing stopwords\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.append('via')\n",
    "        tokens_NoStopWords = [token for token in tokens_NoPunct if token not in stop_words]\n",
    "\n",
    "        # removing links\n",
    "        regex = re.compile('http\\S+')\n",
    "        tokens_NoLinks = [regex.sub('', word) for word in tokens_NoStopWords if not regex.sub('', word) == '' and not word.startswith('tc')]\n",
    "\n",
    "        # Lemmatizing\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_NoLinks]\n",
    "\n",
    "        # Detokenizing\n",
    "        untokenized = TreebankWordDetokenizer().detokenize(lemmatized_words)\n",
    "\n",
    "        processed_Reviews.append(untokenized)\n",
    "    return processed_Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "186567d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_Reviews = processText(df)\n",
    "df['processedReview'] = processed_Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3821310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processedReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...          1   \n",
       "1  A wonderful little production. <br /><br />The...          1   \n",
       "2  I thought this was a wonderful way to spend ti...          1   \n",
       "3  Basically there's a family where a little boy ...          0   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1   \n",
       "\n",
       "                                    processedReviews  \n",
       "0  one reviewer mentioned watching 1 oz episode h...  \n",
       "1  wonderful little production br br filming tech...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically family little boy jake think zombie ...  \n",
       "4  petter mattei love time money visually stunnin...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7e8f8",
   "metadata": {},
   "source": [
    "# Classification Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968efa1",
   "metadata": {},
   "source": [
    "#### Testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef4be93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "# sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'\n",
    "\n",
    "# encoding = tokenizer.encode_plus(\n",
    "#   sample_txt,\n",
    "#   add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#   max_length = 32,\n",
    "#   padding = 'max_length',\n",
    "#   # truncation = True,\n",
    "#   return_attention_mask = True,\n",
    "#   return_token_type_ids = False,\n",
    "#   return_tensors = 'pt',  # Return PyTorch tensors\n",
    "# )\n",
    "\n",
    "# # encoding.keys()\n",
    "# # encoding['input_ids'][0], encoding['attention_mask']\n",
    "# # tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "\n",
    "# bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "# bm = bert_model(\n",
    "#   input_ids=encoding['input_ids'],\n",
    "#   attention_mask=encoding['attention_mask'],\n",
    "# )\n",
    "\n",
    "# print(bm['last_hidden_state'].shape)\n",
    "# print(bm['pooler_output'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b77999",
   "metadata": {},
   "source": [
    "#### Custom Dataset Class & Dataloader Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ddbcb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDBReviewsDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    review = self.reviews[item]\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "        max_length = self.max_len,\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        return_attention_mask = True,\n",
    "        return_token_type_ids = False,\n",
    "        return_tensors = 'pt',  # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "\n",
    "def create_data_loader(features, classification, tokenizer, max_len, batch_size):\n",
    "    dataset = IMDBReviewsDataset(\n",
    "        reviews = features.review.to_numpy(),\n",
    "        targets = classification.to_numpy(),\n",
    "        tokenizer = tokenizer,\n",
    "        max_len = max_len\n",
    "    )\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3330399",
   "metadata": {},
   "source": [
    "#### Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8eab88e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing dataset class\n",
    "# max_len = 500\n",
    "# dataset = IMDBReviewsDataset(reviews = X_train.review.to_numpy(), targets = y_train.to_numpy(), tokenizer = tokenizer, max_len = max_len)\n",
    "# dataset.__getitem__(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc4128db",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "\n",
    "train_data_loader = create_data_loader(X_train, y_train, tokenizer, 250, 16)\n",
    "val_data_loader = create_data_loader(X_val, y_val, tokenizer, 250, 16)\n",
    "test_data_loader = create_data_loader(X_test, y_test, tokenizer, 250, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b4d1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 250])\n",
      "torch.Size([16, 250])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Checking one batch from dataloader\n",
    "data = next(iter(train_data_loader))\n",
    "data.keys()\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcd2da8",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d635b",
   "metadata": {},
   "source": [
    "##### Unused bits (might be useful later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e757f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hidden_sizes = [768, 512, 256, 128, 64]\n",
    "# print(hidden_sizes[-1])\n",
    "\n",
    "# self.layers = nn.ModuleList()\n",
    "    # for h in range(len(hidden_sizes)-1):\n",
    "    #   self.layers.append(nn.Linear(hidden_sizes[h], hidden_sizes[h+1]))\n",
    "    \n",
    "    # self.out = nn.Linear(hidden_sizes[-1], n_classes)\n",
    "\n",
    "# self.out = nn.Linear(self.bert.config.hidden_size, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd1291",
   "metadata": {},
   "source": [
    "##### Classifier NN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2a3e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "\n",
    "\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    \n",
    "    self.out = nn.Sequential(\n",
    "      nn.Linear(768, 512),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(512, 256),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(256, 128),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, n_classes),\n",
    "    )\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    bm = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      \n",
    "    )\n",
    "    output = self.drop(bm['pooler_output'])\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd23c89",
   "metadata": {},
   "source": [
    "##### Testing classifier using single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b44303f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 250])\n",
      "torch.Size([16, 250])\n",
      "tensor([[0.4955, 0.5045],\n",
      "        [0.4986, 0.5014],\n",
      "        [0.5006, 0.4994],\n",
      "        [0.5001, 0.4999],\n",
      "        [0.4963, 0.5037],\n",
      "        [0.4984, 0.5016],\n",
      "        [0.4897, 0.5103],\n",
      "        [0.4999, 0.5001],\n",
      "        [0.5040, 0.4960],\n",
      "        [0.4962, 0.5038],\n",
      "        [0.4981, 0.5019],\n",
      "        [0.4952, 0.5048],\n",
      "        [0.4955, 0.5045],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.4982, 0.5018],\n",
      "        [0.4986, 0.5014]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length\n",
    "\n",
    "# Output tensors\n",
    "out_tensors = nn.functional.softmax(model(input_ids, attention_mask), dim=1)\n",
    "print(out_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550f0f4",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4e4f96c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61ad6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    # print(\"outputs: \", outputs)\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    # print(\"max: \", preds)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    print(\"correct_predictions: \", correct_predictions)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d48d505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a401546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(EPOCHS):\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(X_train)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  # val_acc, val_loss = eval_model(\n",
    "  #   model,\n",
    "  #   val_data_loader,\n",
    "  #   loss_fn,\n",
    "  #   device,\n",
    "  #   len(X_val)\n",
    "  # )\n",
    "  # print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  # print()\n",
    "  # history['train_acc'].append(train_acc)\n",
    "  # history['train_loss'].append(train_loss)\n",
    "  # history['val_acc'].append(val_acc)\n",
    "  # history['val_loss'].append(val_loss)\n",
    "  # if val_acc > best_accuracy:\n",
    "  #   torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "  #   best_accuracy = val_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
